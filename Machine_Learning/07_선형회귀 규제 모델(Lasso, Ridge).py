{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99b0557",
   "metadata": {},
   "source": [
    "## 보스턴 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d30d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6681427f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70811aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df, boston.target,\n",
    "                                                random_state=727,\n",
    "                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0993be4",
   "metadata": {},
   "source": [
    "## 컬럼 증식\n",
    "- 릿지, 라쏘는 선형회귀 모델이 과대적합인 경우에 사용하는 모델이다.\n",
    "- 선형회귀 모델이 과대적합이 되도록 다항회귀 데이터를 만들어보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ff1031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = X_train.columns\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c16601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n",
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_5676\\4102070932.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]\n"
     ]
    }
   ],
   "source": [
    "for c in col_names :\n",
    "    for c2 in col_names :\n",
    "        X_train[c+\" x \"+c2] = X_train[c] * X_train[c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2a23d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 182)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16a8e6",
   "metadata": {},
   "source": [
    "## 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7de52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso # L1 규제모델\n",
    "from sklearn.linear_model import Ridge # L2 규제모델\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad4dd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 점수 : 0.7946879109399452\n",
      "LinearRegression 점수(train) : 0.933171164250107\n",
      "Lasso 점수 : 0.822381478807786\n",
      "Lasso 점수(train) : 0.8859789446114941\n",
      "Ridge 점수 : 0.7834663589110872\n",
      "Ridge 점수(train) : 0.9272567648673558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e+03, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.679e+03, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.842e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.169e+03, tolerance: 3.387e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# 선형회귀모델\n",
    "linear = LinearRegression()\n",
    "linear_score = cross_val_score(linear,X_train,y_train,cv=5)\n",
    "# Lasso 모델\n",
    "lasso = Lasso()\n",
    "lasso_score = cross_val_score(lasso,X_train,y_train,cv=5)\n",
    "# Ridge 모델\n",
    "ridge = Ridge()\n",
    "ridge_score = cross_val_score(ridge,X_train,y_train,cv=5)\n",
    "\n",
    "linear.fit(X_train,y_train)\n",
    "lasso.fit(X_train,y_train)\n",
    "ridge.fit(X_train,y_train)\n",
    "\n",
    "print(\"LinearRegression 점수 :\",linear_score.mean())\n",
    "print(\"LinearRegression 점수(train) :\",linear.score(X_train,y_train))\n",
    "print(\"Lasso 점수 :\",lasso_score.mean())\n",
    "print(\"Lasso 점수(train) :\",lasso.score(X_train,y_train))\n",
    "print(\"Ridge 점수 :\",ridge_score.mean())\n",
    "print(\"Ridge 점수(train) :\",ridge.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255fb4f",
   "metadata": {},
   "source": [
    "## 모델 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7acafad",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127ec175",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.032e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.116e+02, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.419e+02, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.109e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.777e+02, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.139e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.075e+03, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.980e+02, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.371e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+03, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.464e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.830e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e+03, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.679e+03, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.842e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.168e+03, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e+03, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e+03, tolerance: 2.539e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.186e+03, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.159e+03, tolerance: 2.934e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.206e+02, tolerance: 2.777e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.873e+00, tolerance: 2.418e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.719e+00, tolerance: 2.876e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# 규제 파라미터 값을 작은 값 -> 큰 값\n",
    "lasso_score = []\n",
    "for a in [0.001,0.01,0.1,1,10,100,1000]:\n",
    "    m = Lasso(alpha=a)\n",
    "    rs = cross_val_score(m, X_train,y_train, cv=5)\n",
    "    lasso_score.append(rs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb85307",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb1c937b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=5.0543e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.89667e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.87021e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.56326e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "C:\\Users\\AI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.96073e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# 규제 파라미터 값을 작은 값 -> 큰 값\n",
    "ridge_score = []\n",
    "for a in [0.001,0.01,0.1,1,10,100,1000]:\n",
    "    m = Ridge(alpha=a)\n",
    "    rs = cross_val_score(m, X_train,y_train, cv=5)\n",
    "    ridge_score.append(rs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f91ff",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63ecef95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAE9CAYAAABOT8UdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA+YUlEQVR4nO3deXyU5b3//9cnk8m+sAQCCWCCBBBUUDaxqEEFtVWxfov7vmJr3c7paT39nR7PaU972lq1HFGk7ssRl256amtFjVRFEXc22ZcAskP2/fr9MZOVBCYwk3smeT8fj/sxc99z3zOfXAnJm+u+7us25xwiIiIiEh3ivC5ARERERJopnImIiIhEEYUzERERkSiicCYiIiISRRTORERERKKIwpmIiIhIFIn3uoBwysrKcnl5eRH9jPLyclJTUyP6GT2J2jP81KbhpzYNL7Vn+KlNw6ur2vPjjz/e5Zzr13Z7twpneXl5LFmyJKKfUVRURGFhYUQ/oydRe4af2jT81KbhpfYMP7VpeHVVe5rZxva267SmiIiISBRROBMRERGJIgpnIiIiIlGkW405ExERkehTW1tLcXExVVVVXpcSkszMTFasWBG290tKSmLQoEH4/f6Q9lc4ExERkYgqLi4mPT2dvLw8zMzrcg6ptLSU9PT0sLyXc47du3dTXFxMfn5+SMfotKaIiIhEVFVVFX379o2JYBZuZkbfvn071WuocCYiIiIR1xODWaPOfu0KZyIiItLtpaWleV1CyBTORERERKKILggQEWlHVW09X31dyuodZawormX/Z1tI8MXh98WREN/8mNC0bvh9cSTGt97H77MefTpHJJq9+uqr/OxnP6Ompoa+ffvy3HPPkZ2dzbvvvsvdd98NBE5JLly4kLKyMi6++GJKSkqoq6vj4Ycf5pRTTuH555/n5z//Oc45vvWtb/HLX/7yiOtSOBORHm9veQ3Lt5WwbOt+lm8tYdnWEtbuLKPBtdhp6WeH/f5tA1zrUHfwcNe4r99nHYTC4OvxcSS03Kdp24Gfk+jz4Y83Enxx+OIUHqXnmjJlCh988AFmxqOPPsqvfvUrfvOb3zB79mzmzJnDN77xDcrKykhKSmLevHmcddZZ/PjHP6a+vp6Kigq2bt3KD3/4Qz7++GN69+7N9OnT+dOf/sQFF1xwRHUpnIlIj+Gco3hvJcu2lrB8636Wbyth+dYStu5vvopqYGYSowZmcM6xAxiVk8GIARksWfwhJ06YSG19AzV1DcFHR019A7V1DYHH+gaqm14LPNbWuwO21TTt76ipqw8+BraVVtWx5xDH17VKjEfOjEAobBHmGoNb63DYHPIS41uHxY5DpbUbNHdVNIT1a5DY8h+vLmP51pKwvueonAz+/bzRnT6uuLiYiy++mG3btlFTU9M01cVJJ53EXXfdxeWXX86FF17IoEGDmDBhAtdddx21tbVccMEFjB07lrfeeovCwkL69Qvcu/zyyy9n4cKFCmciIu2prW9g9fayVj1iy7eVUFpVB0CcwdH90piQ34fRORmMGpjJqJwM+qQmHPBeG1PiOLpfdAwmbmgIhMLGYNgy3DUFwBahsaZxn/p6ausc1S0DZXD/wLbmfWravEdjOKyorG8VMmvbCaWhZMeEOOh39A4KR/SPfIOJHMT3v/997rrrLs4//3yKioq45557ALjrrru48MILee211zjppJNYsGABp556KgsXLuQvf/kLV155JT/4wQ/IyMiISF0KZyIS80qralmxrbSpN2zZ1hJWby+jpj7QQ5Ps9zFyYDrnj8lhdE4ghI0ckE6S3+dx5Z0XF2ckxfmitvb6Bteid/DAHsPKmnr+6X8/5Manl3DfRWM5b0yO1yVLFzucHq5I2b9/P7m5uQA89dRTTdvXrVvHmDFjOO6441i0aBErV64kOTmZ3NxcbrzxRsrLy/nkk0/44Q9/yO23386uXbvo3bs3zz//PN///vePuC6FMxGJGc45dpRWt+oJW7a1hI27K5r26ZuawKicDK6dkhcIYgMzyM9KxRencVVdwRdnJCf4SKbj8PjDCUk8sTaR2+Z/Sll1HZdOHNKFFUpPVVFRwaBBg5rW77rrLu655x5mzpxJbm4uJ510EuvXrwfgoYce4r333sPn8zFq1CjOOecc5s+fz69//Wv8fj9paWk8/fTTDBw4kF/84hdMnToV5xzf/OY3mTFjxhHXqnAmIlGpvsGxfld569OSW0vYXV7TtM9RfVMYnZPBzHGDGJWTweicTPqnJ2qAe5RL8RtPXzeJW577mLv/8CX7K2uZddrRXpcl3VxDQ/tjHdsLU/fee+8Bt2+6+uqrufrqqw/Y97LLLuOyyy4LT5FBCmci4rmq2npWfl0avFIycGpy5bZSKmvrAfD7jOHZ6Zw+sn9gfFhOJscMTCc9KbSbCEv0SU7wMe/K8fzTS5/z339dyf7KWv7lrBEK1iIonIlIF9tTXhM8Jbk/eNVk62kr0pPiGTUwg0smDm46LTmsfxoJ8Zozu7tJiI/jgYvHkp4Uz8NFaymprOU/ZxyrU9DS4ymciUhENE9b0Tx32PJtJWxrM23F6JzGaSsyGZ2TwaDeyeo96UF8ccZ/XXAsmcn+QECrquO+i8bg9ymMS8+lcCYiR6ymroE1O8qaTkl2NG3FpPw+TWPDjhnY/rQV0vOYGT88eySZyX7++68rKauq5aHLx5GcEJ1XpIpEmsKZiHRK47QVLa+YbG/aihljc5rmDovVaSuka8067Wgyk/386x+/5OrHF/PoNePJ0LhC6YEUzkSkXc45tpdUs3xb69OSmrZCIunSiUNIT4rnzhc+49J5H/DUdRPJSkv0uiyRLqVwJiJN01a0Oi3ZZtqKvBbTVjRO5KppKyQSzj0+h7TEeGY9+zEXzV3EMzdMIrdXstdlSYzz+Xwcd9xx1NXVkZ+fzzPPPEOvXr3YunUrt912Gy+//PIBxxQWFnLvvfcyfvz4Lq1V4Uykh6msqeer7aWtBuqv/LqEqtrAacnGaSvOOKY/owZmMDo3k5EDNG2FdK3CEf155vpJXPfkR8x8+H2evWESQ6PkFloSm5KTk/nss8+AwJxlc+bM4cc//jE5OTntBjMvKZyJdGON01Ysa3Fbo3XtTFtx2cSjggP1Mzi6n6atkOgwIa8P8286iaseW8zMuYt46rqJHJub6XVZ0g1MnjyZL774AoANGzZw7rnnsnTpUiorK7n22mtZunQpo0ePprKysumYxx57jF/+8pfk5ORQUFBAYmIiDz74IDt37mTWrFls2rQJgAceeIBvfOMbR1SfwplIN1BWXceGXeWs31XOG6treGbDRwdMW5GTmcSonAy+edzAQI+Ypq2QGDA6J5OXZk3myscWc+m8D3jsmglMzO/jdVkSw+rr63nzzTe5/vrrD3jt4YcfJiUlhUWLFrF+/XpOPPFEALZu3cpPf/pTPvnkE9LT0zn99NMZM2YMALfffjt33nknU6ZMYdOmTZx11lmsWLHiiGpUOBOJEVW19WzcXcH6XWWs31XRFMbW7y5nZ2l1034GFGRXaNoK6TaG9kvjpVmTueKxD7nq8Q95+PJxTB3Z3+uy5HD99Ufw9Zfhfc8Bx8E5/33QXSorKxk7diwbNmxg3LhxTJs27YB9Fi5cyG233QbA8ccfz/HHHw/A4sWLOe200+jTJ/Afg5kzZ7Jq1SoAFixYwPLly5veo6SkhNLS0gNu/9QZCmciUaSmroHNe1sEr13lbNhdzvqd5WwrqcK55n2z0hLIz0qlcHg/8rJSGZqVSl5WKpuXf8z0M07z7osQiYCcXsm8dPNkrn5iMTc+vYT7Lx7LeWNyvC5LYkjjmLP9+/dz7rnnMmfOnKYg1lJ7ZxNcy1++bTQ0NLBo0SKSk8N30YrCmUgXq29wbNlbyfrd5azfWcaG3RVNQWzLvkrqG5p/CWQm+8nLSmXS0L7k9U0lLyuFoVlpHJWV0uH8T9u/0mlK6Z76piXyvzeexA1PLeG2+Z9SWlXHZZOGeF2WdNYhergiLTMzk9mzZzNjxgxuueWWVq+deuqpPPfcc4wfP56lS5c2jUubOHEid955J3v37iU9PZ3f//73HHfccQBMnz6dBx98kB/84AcAfPbZZ4wdO/aIalQ4E+84hzXUeV1FRDQ0OL4uqWLDrnLW7SpnQ2MP2K5yNu2poLa+OYClJvjIy0rl+EGZzBibEwxhgZ6w3joVKdJKRpKfp6+byHef+4R//eOX7K+s5ZbCo70uS2LMCSecwJgxY5g/fz6nnHJK0/ZbbrmFa6+9lsmTJ3PiiScyceJEAHJzc/nXf/1XJk2aRE5ODqNGjSIzM3BxyuzZs/ne977H8ccfT11dHaeeeipz5849ovoiGs7M7Gzgt4APeNQ5999tXs8EngWGBGu51zn3RCjHSoQ0NEBdJdQGl7oqqK2A2uBjXVWL1yqD21s+r2h9TNN7VbV+3+B7n4aDRSmQ3AdSegcf+0JKn+Dz4Hrb1xPTweOB7M45dpZVsyE4/qtlCNuwu7xpagqAxPg48vqmMqx/GtNGDSA/K4W8vqnk90ulX5rmChPpjCS/j0euHMc/vfg5v/zbSvZX1vLDs0fo35EcVFlZWav1V199ten50qVLgcCpz/nz57c7Zuyyyy7jpptuoq6ujm9/+9tMnz4dgKysLF544YWw1hqxcGZmPmAOMA0oBj4ys1ecc8tb7PY9YLlz7jwz6wd8ZWbPAfUhHNtz1Ne2E4jahqD2AlFHx3QUtCqhvubQ9bTHfOBPAX8SxCeDPznw3J8CCWmQ2i+wLb5xe+D5+k3F5A/sAxV7oHJP4HHb54HnlfuADs7zx8W3CW+9WwS6NuGucVtyL4jr/C2E9pbXsH53+QHjwDbsqqCsurnnz+8zBvdJIb9vKlOGZZGXlUp+cBzYwIwk4jRrvkjY+H1xPHDxWDKS45n7zlr2V9byswuO1d0pJGLuueceFixYQFVVFdOnT+eCCy6I2GdFsudsIrDGObcOwMzmAzOAlgHLAekW+O9OGrAHqAMmhXCsd5yDuup2Ak57vUoh9iS16qFqc7yrP7w6fQltAlEKxAeDUUqf4PO2gSq59famY9oc3/S+wcV3eBOUbiwqIr+wsP0XG+qhaj9U7G4d3ip2Nz9vfNy9Foo/CjxvqO3g0wySMtv0xgXCW3VCL3bVp7K1NoXNlUmsKUtgxX4/S/f62Nk8zQ1xBoN6p5CXlcq4Ib2bwld+Viq5vZKJ92l+MJGuEhdn/HTGsWQk+XmoaC2lVbXcd9FYzdMnEXHvvfd22WdFMpzlAptbrBcTCF0tPQi8AmwF0oGLnXMNZhbKsV3v8bM5pfgTKKqhwx6dQ+koBMUnQdqAg4SgpAMDUUfhqvG1w+gliipxvmCQ6sScRs5BTVmbQLcXKnZTW7aL0r07qNq/k/ry3diudSTWfEpqQwkpVJFL4Id2Qpu3rE1Npi6xD5bah4S0vsSl9m0Od/F9oLoP7A0+Nga+hDTPT7uK9ARmxr+cPZLMZD+/+OtKyqrrePjycSQnxPjvP+nRIhnO2vvL1DbRnAV8BpwOHA28YWb/CPHYwIeY3QTcBJCdnU1RUdFhlntog+OHQ7/e+JLSaIhLpCEugXpf4LHl88Djga83xCUc3h9sB9QElwNUBpe9R/KleaasrCys37O6BseOCsf2iga+Lg88bi9PY3tFKnuqBrfaNyPBGJBqZKfEkZtSS15CObkJ5QyILyOlvhR/bQnxdaX4awPP/bWl1OzaSvzXXwW21ZV1UAU0WDy1/nRq/RnUxac3PQ88tt3euJ4KduT/4w93m4raNNwi0Z4jgGtHJ/Dksp3MuP/v3H5iEqn+nvMfpGj/Gc3MzKSkpCRmxgXW19dTWloatvdzzlFVVRXy9yiS4awYaPnXcBCBHrKWrgX+2wUmEFljZuuBkSEeC4Bzbh4wD2D8+PGusKNTZGEw5+1BfLZyLUcPOIqE+DgSWyyBdV/T9sb11KbnB+6TGB8XMz+okVJUVERnv2d19Q1s2VfZPAC/cTD+7nK27K2kxUwU9Erxk5+VQeFRgdOPjVdBHtU35cjvFVlfB1X7WvTQNffWxVXsIbFiN4mVe5tfL1kTeN7RaWqLg6Re7Yyd693uqdimsXbxra/oPJw2lYNTm4ZXpNqzEBj/xTbueOFT5iyP5+nrJ5KVlhj2z4lG0f4zun79empqaujbt29M/N070klkW3LOsXv3bnr16sUJJ5wQ0jGRDGcfAQVmlg9sAS4BLmuzzybgDOAfZpZN4D8/64B9IRzb5Rau2skXW+pYuGU91XUNhz4gBAm+uFZhrf2Q116wO3Cf9rf7SPTHkeCLa/Hoa70ehSGxocGxraSK9TvLmwbjNw7I37y39VQUaYnx5GWlMHZwb749NrdpDFh+Viq9UiI4FYUvHlKzAkuonIPqkuDYuT0Hjp1rOZ6upBi+/iLwvK6y4/dMSA9cyRoMb8Mr4iBpBfQfFVjS+h351yoSI751/EDSkuK5+ZklXDR3Ec/cMIncXuGbHFQOz6BBgyguLmbnzp1elxKSqqoqkpKSwvZ+SUlJDBo0KOT9IxbOnHN1ZnYr8DqB6TAed84tM7NZwdfnAj8FnjSzLwmcyvyhc24XQHvHRqrWUL1w8+Sm/50456itd1TX1VNd10BNXUOLx/oD1quD6wfbp3G9uraBmvrmbWXVdQe+X20D1fWB9XBoDGlNQa9tgGsKcqGFv8QDjg3s1zZo7q5s4MN1u5tuQ9QYwDburmgVgJP8gakohmenc9axA8jv2zwQPystIerCZYcseFFCUib0yQ/9uNrKji+GaBPusnatgb+90Xxsaj/ofwz0Hx14zB4N/UZCYlr4vz6RKHDa8H48e/0krn3yI77z8Ps8c/0khvXXz7uX/H4/+fmd+J3nsaKiopB7uSIhovOcOedeA15rs21ui+dbgemhHhtNzIyEeCMhPo7wdHweHudcMMh1HBADYa++OfQFg111bX3z+sFCY3Cf0qq64L5t9wm872F75wMgMBXFkD4p5GelclrwlkT5wbnAstN7+FQU/mTIzA0sh/D+229TOGE0bF8GO5YHlu3L4ZOnAlf/Nup1VCCo9T+muZctq+Cwr7wViSbj8/rwwk2TuerxD7nokUU8fd1Ejs3N9LoskZDoDgExzswCvVfx3l6Z1NAQCInNYa+dANdO+Fu+ciVTJ40lv28qOb2SNBVFOJhBWv/AcvTU5u0NDbBvA+xYEQhrjcFt1evN4+Di/IGA1n9Ucy9b/2MgcwjE6XsjsWVUTgYvzTqZKx79kEvnfcCjV49n0tC+XpclckgKZxIWcXFGUpyPJL8POnGavqh8LacN15ioLhEXB32GBpaR32reXlcNu1YHQtuOZYHgtnkxLH25eZ+EtMCp0OxRrU+Pdma8nYgH8rNSefmWyVzx6Idc9fhi5l4xjqkj+3tdlshBKZyJ9HTxiTDg2MDCzObtVSWwc2Xw9OiKQC/biv+DT55u3ie1X/Mp0ezgo8azSZQZmJnMizdP5ponPuLGp5fwm4vGMGPsoYcIiHhF4UxE2peUAYMnBpZGzkHZjtZj2XaEMJ4tezT0HabxbOKZvmmJ/O+Nk7j+qSXc8cJnlFbVccVJR3ldlki7FM5EJHRmkJ4dWNobz7Z9efPp0R0r2hnPNjx4SnRUc49b5mCNZ5MukZ7k5+nrJvK95z7h//vTUkqqavlu4TCvy5IoZA11h94pghTOROTItRzPdsy5zdubxrO16GlrbzxbyytGG4ObxrNJBCT5fcy9chz//NLn/OpvX7G/spYfnT0ydqbjkfBwDsp3wt4N7S4nV5TA6cWeladwJiKR02o8WwtVJc3j2HYEe9tWvBo4PdootX/rK0b7j4Z+IzSeTY6Y3xfH/ReNJSPJzyPvrKOkspafXXAcvp48XU93VFsJ+zZ1GMBaDcUASM+B3nmQfxrF+x35DfWe3aNa4UxEul5SBgyZFFgaNY1nW9Ziuo9lsOSJ1ndI6J13YC+bxrNJJ8XFGf85YzSZyX4efHsNJZV13H/xWBLidYo9ZjgHZds7Dl+l21rv70+B3vmBZejUwO+SxqXX4MB8kkEbi4rI9yiYgcKZiESLVuPZTm/e3tAAe9e37mnb3t78bMODYa3F3RB6DQm8r0g7zIx/PmsEmcl+/uu1FZRW1zH3ihNJSdCfxqhRUwH7NnYQwDa2ubWdQUaw9+vo01uEr/zAY2pWzPw+0E+giES3uDjoe3RgOWA826pgL1uwt23TB/DlS837JKRD/5FtetpGQ6omIpVmN546lIzkeO7+w5dc9dhiHrtmApnJ6ontEg0NUPZ1x71fZdtb75+QFghafYfBsDNb935lDgZ/+O6H6SWFMxGJTfGJMOC4wNJS1X7YsbL16dEVrxw4nq3lFaP9RwVCXEJq134NEjUunjCE9CQ/t8//lEvmfcDT102kX3qi12V1D9VlB+/9qq9usbNB5qBA2CqY1rrnq3cepPSNmd6vI6FwJiLdS1JmB+PZtreYm21FO+PZDHof1eIOCC3Gs0mP8M3jBpKWGM/Nz3zMRY8s4pnrJzKod4rXZUW/hgYo3dpx71f5ztb7J6RDn7zABT7Dz2p9+jFzMMQndGn50UjhTES6PzNIHxBYWo1nqw/88Wi8YrTx9Oiqv7UYzxbPyb5U+LI/JPeCpF4dPGYeuC0hrUf8L787OXV4P569YSLXPvERM+cu4pnrJzGsv64Qpro00Mt1QPhaH7gisr6meV+La+79GnFO61OPvfMhubf+XRyCwpmI9Fxxvhbj2c5r3l5bBbtXB3rZdn3FrjVLyemTApX7oGIX7F4DVfsCp1Bdw0HeP7790JbUK7D9YGEvIV2T83pk3FF9eOHmyVz52GIuemQRT107keMGZXpdVmQ11EPJQXq/Kna13j8xM9D7lT06cK/etmO/dPX0EVE4ExFpy5/UajzbKl8ROYWFB+7X0AA1pYHQVrWvg8f9rbft3dC83tg71x6LaxPsOgh57T0mZirYHaFjBmbw0qzADdMv/d0HPHb1eCYNjfELSapKOg5f+zZBQ23zvuYLTC/ROy9wIU6r3q+8QO+XRIzCmYjI4YprDFCZQCfv0+gc1JR1Ltjt39K83vIP6QEMEjMguROBLqlXc4+eT38aAPKzUvn9LSdzxWMfctXji3n4ihM5fWS212V1rL4OSrZ0HMAq97TeP7l3IGgNPB5Gnd86fGUM0s+Bh9TyIiJeMIPE9MDC4M4d61xgdvOOgl3V/gO37fyqeb3V1XHtSEg/cDxde0GuvZAXy6eznAuc3muoa1oGxNfz0uVDuWv+x/zn03+l4ZsFnDkiq9U+TcfU17Zeb/t6Q10gVB/wemfeo53X62qYtH0NLNwVWG8UFx+Y6693HuRc0GbS1aMC3y+JSgpnIiKxxiww7UdCKmTmdv742srOBbvda5vXW0362Q5/6iHH1w3YtgmWrAshkBwilBxpqGlvvR29gScAEoAFwSUSLC4QqJoWX2CC5VbrbV73BV4vTR9G8sjLWwew9Bz1fsUofddERHoaf3JgyRjY+WPrqkM7BdveGLvacgBGAnx1kM84IKS0XdqEFF+b1+MTQj/e528/9DQ9b3691sXx1IfFfL6ljG+NHcxZx+VivoR2jm/5Hv523rPFejBcYb4jGie4vKiI/u2Ni5SYpHAmIiKhi09svs1WZ9XVQNV+Fr37NpNPPuWA3p9whJRI8gNXn9DAv7z8BbM+2cJNaUO5+5yRmKaFkDBTOBMRka4RnwBp/ahO6n94vXZRwO+L4zczx5CeFM+8hevYX1HLzy88Dl+cApqEj8KZiIhIJ8TFGf9x/mh6JfuZ/dYaSqtruf/isSTG+7wuTboJhTMREZFOMjPumj6CjGQ/P/vLCkqrlvDIleNISdCfVTly0XliX0REJAbccMpQfvX/jue9Nbu48rHF7K842PxzIqFROBMRETkCF00YzJzLTuTL4v1cPG8RO0sPMY+cyCEonImIiByhc44byGPXjGfj7gpmzn2fzXsqvC5JYpjCmYiISBicUtCPZ2+YxJ7yGmbOXcSaHaVelyQxSuFMREQkTMYd1ZsXZ02m3jlmzl3EF8X7vC5JYpDCmYiISBiNHJDBSzdPJjUxnst+9yGL1u72uiSJMQpnIiIiYZaXlcrLs05mYGYSVz+xmAXLt3tdksQQhTMREZEIGJCZxIs3T+aYAenc/OzH/OnTLV6XJDFC4UxERCRCeqcm8NyNJzExrw93vPAZTy/a4HVJEgMUzkRERCIoLTGeJ66dwLRR2fzkz8t48K3VOOe8LkuimMKZiIhIhCX5fTx8+YlceEIu9/59FT9/bYUCmnRINwETERHpAvG+OO6dOYaMZD+/+8d69lfW8osLj8cXZ16XJlFG4UxERKSLxMUZ/37eKDKS/cx+czWlVXU8cMlYEuN9XpcmUUSnNUVERLqQmXHXtOH827mj+OvSr7nhqSVU1NR5XZZEEYUzERERD1w/JZ9ff+d43luziyse/ZD9FbVelyRRQuFMRETEIzPHD+ahy09k6ZYSLp63iB2lVV6XJFFA4UxERMRDZx87kMevmcCmPRXMnLuIzXsqvC5JPKZwJiIi4rEpBVk8e8Mk9lXU8p2577N6e6nXJYmHFM5ERESiwIlDevPCzSfR4OCiRxbx+eZ9XpckHlE4ExERiRIjB2Tw8qzJpCXFc9nvPmDR2t1elyQeUDgTERGJIkf1TeXlWSeT0yuZq59YzBvLt3tdknQxhTMREZEok52RxIs3T+aYgRnMevZj/vhpsdclSReKaDgzs7PN7CszW2NmP2rn9R+Y2WfBZamZ1ZtZn+BrG8zsy+BrSyJZp4iISLTpnZrAczdMYlJ+H+584XOeen+D1yVJF4lYODMzHzAHOAcYBVxqZqNa7uOc+7VzbqxzbixwN/COc25Pi12mBl8fH6k6RUREolVaYjyPXzOBaaOy+fdXljH7zdW6YXoPEMmes4nAGufcOudcDTAfmHGQ/S8Fno9gPSIiIjEnye/j4ctP5MITc7nvjVX87C8rFNC6OYvUN9jMvgOc7Zy7Ibh+JTDJOXdrO/umAMXAsMaeMzNbD+wFHPCIc25eB59zE3ATQHZ29rj58+dH4stpUlZWRlpaWkQ/oydRe4af2jT81KbhpfY8PA3O8fzKGt7YWMcpufFcMzoBX5wBatNw66r2nDp16sftnR2Mj+BnWjvbOkqC5wHvtTml+Q3n3FYz6w+8YWYrnXMLD3jDQGibBzB+/HhXWFh4hGUfXFFREZH+jJ5E7Rl+atPwU5uGl9rz8E0tdPz2zdU8sGA1qb2y+O2lY0mM96lNw8zr9ozkac1iYHCL9UHA1g72vYQ2pzSdc1uDjzuAPxI4TSoiItJjmRl3nDmcn5w7ir8t+5rrn1xCeXWd12VJmEUynH0EFJhZvpklEAhgr7TdycwygdOAP7fYlmpm6Y3PgenA0gjWKiIiEjOum5LPvTPH8P7aXVzx2IeU12oMWncSsdOazrk6M7sVeB3wAY8755aZ2azg63ODu34b+LtzrrzF4dnAH82sscb/dc79LVK1ioiIxJrvjBtEelI833vuE36Pj29N87oiCZdIjjnDOfca8FqbbXPbrD8JPNlm2zpgTCRrExERiXVnjR7AzPGDeemjTWzZV0lur2SvS5Iw0B0CREREYtitpw8D4MG31nhciYSLwpmIiEgMy+2VzGmD43lpyWY276nwuhwJA4UzERGRGHfuUD9xccbsN1d7XYqEgcKZiIhIjOudFMcVk47iD59uYf2u8kMfIFFN4UxERKQbmFU4FL/P+B/1nsW8kMKZmU0xs2uDz/uZWX5kyxIREZHO6J+exFWT8/jTZ1tYs6PM63LkCBwynJnZvwM/BO4ObvIDz0ayKBEREem8m08dSpLfx2/VexbTQuk5+zZwPlAOTbdVSo9kUSIiItJ5fdMSuebkPP7vi6189XWp1+XIYQolnNU45xzBm5YHb6ckIiIiUejGU4aSmhDPb99c5XUpcphCCWcvmtkjQC8zuxFYAPwusmWJiIjI4eidmsB1U/J57cuvWbZ1v9flyGE4aDizwM0tXwBeBn4PjAB+4pz7ny6oTURERA7D9VPySU+K54EFGnsWiw56b03nnDOzPznnxgFvdFFNIiIicgQyk/3ceMpQ7ntjFV8W7+e4QZlelySdEMppzQ/MbELEKxEREZGwufYbefRK8XPfG195XYp0UijhbCqBgLbWzL4wsy/N7ItIFyYiIiKHLz3Jz02nDuXtr3byyaa9XpcjnRBKODsHGAqcDpwHnBt8FBERkSh29eQ8+qQmcP8bunIzlhwynDnnNgK9CASy84BewW0iIiISxVIT45l12lD+sXoXH23Y43U5EqJQ7hBwO/Ac0D+4PGtm3490YSIiInLkrjwpj6y0RO77u3rPYkUopzWvByY5537inPsJcBJwY2TLEhERkXBITvDx3cKjWbRuN++v3eV1ORKCUMKZAfUt1uuD20RERCQGXDZpCNkZidz/xioCN/2RaBZKOHsC+NDM7jGze4APgMciWpWIiIiETZLfx61Th/HRhr28u0a9Z9EulAsC7gOuBfYAe4FrnXMPRLguERERCaOLJgwmJzOJ+9R7FvVCuSDgJGC1c262c+63wBozmxT50kRERCRcEuN93Hp6AZ9u2kfRqp1elyMHEcppzYeBshbr5cFtIiIiEkNmjh/E4D7JGnsW5UK6IMC1+A465xo4xD05RUREJPr4fXF8//QCvijez4IVO7wuRzoQSjhbZ2a3mZk/uNwOrIt0YSIiIhJ+F56QS17fFO57YxUNDeo9i0ahhLNZwMnAFqAYmATcFMmiREREJDLifXHcfmYBK7aV8Pqyr70uR9oRytWaO5xzlzjn+jvnsp1zlznn1BcqIiISo84fk8vQfqncv0C9Z9EolKs1f2VmGcFTmm+a2S4zu6IrihMREZHw88UZd5w5nFXby/jLl9u8LkfaCOW05nTnXAlwLoHTmsOBH0S0KhEREYmoc48byPDsNB5YsIp69Z5FlVDCmT/4+E3geeecbmsvIiIS4+LijDvPHM7aneW88vkWr8uRFkIJZ6+a2UpgPPCmmfUDqiJbloiIiETaWaMHcMzADH67YDV19Q1elyNBoVwQ8CNgMjDeOVcLVAAzIl2YiIiIRFag96yADbsr+MOn6j2LFqH0nOGc2+ucqw8+L3fO6dpbERGRbmDaqGyOy81k9purqVXvWVQIKZyJiIhI92Rm3DVtOMV7K3n542KvyxEUzkRERHq8whH9GDu4Fw++tYbqunqvy+nxQpnnzMzsCjP7SXB9iJlNjHxpIiIi0hXMjH+aPpwt+yp58aPNXpfT44XSc/YQgQsCLg2ulwJzIlaRiIiIdLkpw7KYkNebB99eQ1Wtes+8FEo4m+Sc+x7B6TOcc3uBhIhWJSIiIl3KzLhz2nC2l1Tz/OJNXpfTo4USzmrNzAc4gOA8Z7qcQ0REpJs5+egsJg/ty5y311JZo94zr4QSzmYDfwT6m9l/Ae8CP49oVSIiIuKJO6cNZ1dZNc9+sNHrUnqsg4YzM4sD1gP/AvwC2AZc4Jx7qQtqExERkS42Mb8PpxRkMfedtZRX13ldTo900HDmnGsAfuOcW+mcm+Oce9A5t6KLahMREREP3DltOLvLa3hq0QavS+mRQjmt+Xcz+39mZhGvRkRERDx34pDeTB3Rj3kL11FaVet1OT1OKOHsLuAloNrMSsys1MxKIlyXiIiIeOjOacPZV1HLk+9t8LqUHieUG5+nO+finHMJzrmM4HpGKG9uZmeb2VdmtsbMftTO6z8ws8+Cy1IzqzezPqEcKyIiIpFz/KBeTBuVze/+sY79leo960oh3b7JzHqb2UQzO7VxCeEYH4HJas8BRgGXmtmolvs4537tnBvrnBsL3A2845zbE8qxIiIiEll3nFlASVUdj7273utSepRQbt90A7AQeB34j+DjPSG890RgjXNunXOuBpgPzDjI/pcCzx/msSIiIhJmo3MyOefYATz+7nr2VdR4XU6PEUrP2e3ABGCjc24qcAKwM4TjcoGWN+gqDm47gJmlAGcDv+/ssSIiIhI5d5w5nPKaOuYtXOd1KT1GfAj7VDnnqswMM0t0zq00sxEhHNfe1Z2ug33PA95zzu3p7LFmdhNwE0B2djZFRUUhlHb4ysrKIv4ZPYnaM/zUpuGnNg0vtWf4RbpNJ2b7eOwfaxkZt42MhO4/eYPXP6OhhLNiM+sF/Al4w8z2AltDOQ4Y3GJ90EGOu4TmU5qdOtY5Nw+YBzB+/HhXWFgYQmmHr6ioiEh/Rk+i9gw/tWn4qU3DS+0ZfpFu00Gjyph+/zssqxvA3dOPidjnRAuvf0ZDuVrz2865fc65e4B/Ax4DLgjhvT8CCsws38wSCASwV9ruZGaZwGnAnzt7rIiIiETesP5pzBiby1OLNrCztNrrcrq9UC4IGNK4ELiV02fAgEMd55yrA24lcAHBCuBF59wyM5tlZrNa7Ppt4O/OufJDHRv6lyUiIiLhdNsZBdTWOx4uWut1Kd1eKKc1/0JgvJcBSUA+8BUw+lAHOudeA15rs21um/UngSdDOVZERES8kZ+VyoUn5PLshxu5+bShZGckeV1StxXKac3jnHPHBx8LCExz8W7kSxMREZFoctsZBTQ0OB56e43XpXRrIU1C25Jz7hMCU2uIiIhIDzK4Twozxw/m+cWb2bKv0utyuq1Qxpzd1WL5ZzP7X0Kb50xERES6mVtPH4bDMUe9ZxETSs9ZeoslkcAYNM3WLyIi0gPl9krmkglDePGjzWzeU+F1Od3SIS8IcM79R1cUIiIiIrHhe1OH8cKSzfzPW6v51XfGeF1Ot3PIcGZmB51fzDl3fvjKERERkWg3IDOJyycN4elFG/lu4TDyslK9LqlbCeW05nqgEvhdcCkDlgK/CS4iIiLSw9xSeDR+nzH7rdVel9LthBLOTnDOXeycezW4XAZMcc6945x7J9IFioiISPTpn57EVZPz+NOnW1izo8zrcrqVUMJZPzMb2rhiZvlAv8iVJCIiIrHg5lOHkuT3MftN9Z6FUyjh7E6gyMyKzKwIeBu4I5JFiYiISPTrm5bI1Sfn8eoXW1m1vdTrcrqNUO4Q8DegALg9uIxwzr0e6cJEREQk+t10ylBSE+J5YMEqr0vpNkKZhHYmkOCc+xw4D3jezE6MeGUiIiIS9XqnJnDdN/J47cuvWb61xOtyuoVQTmv+m3Ou1MymAGcBTwEPR7YsERERiRXXTxlKepJ6z8IllHBWH3z8FvCwc+7PQELkShIREZFYkpni54YpQ/n78u18Wbzf63JiXijhbIuZPQJcBLxmZokhHiciIiI9xHVT8shM9nO/es+OWCgh6yLgdeBs59w+oA/wg0gWJSIiIrElPcnPTacO5a2VO/h0016vy4lpoVytWeGc+4NzbnVwfZtz7u+RL01ERERiyTUn59EnNYH7F2jesyOh05MiIiISFqmJ8cw6bSgLV+1kyYY9XpcTsxTOREREJGyuPCmPrLRE7ntDY88OV6fDmZn5zOzySBQjIiIisS05wccthUfz/trdLFq72+tyYlKH4czMMszsbjN70MymW8D3gXUELhIQEREROcDlk4aQnZHI/W+swjnndTkx52A9Z88AI4AvgRuAvwPfAWY452Z0QW0iIiISg5L8Pr43dRiLN+zhvTXqPeusg4Wzoc65a5xzjwCXAuOBc51zn3VJZSIiIhKzLp4wmJzMJO574yv1nnXSwcJZbeMT51w9sN45p1vOi4iIyCElxvu49fQCPtm0j6JVO70uJ6YcLJyNMbOS4FIKHN/43Mx0Z1MRERE5qO+MG8Sg3skae9ZJHYYz55zPOZcRXNKdc/Etnmd0ZZEiIiISexLi47jt9AK+KN7Pmyt2eF1OzNA8ZyIiIhIx3z4xl6P6pnDfG6toaFDvWSgUzkRERCRi/L44bj+jgOXbSvj78q+9LicmKJyJiIhIRJ0/Joeh/VK5/43V6j0LgcKZiIiIRFR8sPfsq+2lvLZ0m9flRD2FMxEREYm4c4/PYXh2Gg8sWE29es8OSuFMREREIs4XZ9xx5nDW7Cjj1c+3el1OVFM4ExERkS5x9ugBjByQzm/fXE1dfYPX5UQthTMRERHpEnFxxl3ThrN+Vzl//HSL1+VELYUzERER6TLTRmVzbG4Gs99aTa16z9qlcCYiIiJdxizQe7Z5TyW//7jY63KiksKZiIiIdKmpI/ozdnAv/uetNVTX1XtdTtRROBMREZEu1dh7tmVfJS8uUe9ZWwpnIiIi0uVOKchi/FG9mfPWGqpq1XvWksKZiIiIdDkz467pw/m6pIrnF2/yupyoonAmIiIinjj56CxOGtqHh4rWUlmj3rNGCmciIiLimbumjWBnaTXPfbjR61KihsKZiIiIeGZifh9OKcji4aK1lFfXeV1OVFA4ExEREU/dceZwdpfX8PQi9Z5BhMOZmZ1tZl+Z2Roz+1EH+xSa2WdmtszM3mmxfYOZfRl8bUkk6xQRERHvjDuqN4Uj+vHIwrWUVtV6XY7nIhbOzMwHzAHOAUYBl5rZqDb79AIeAs53zo0GZrZ5m6nOubHOufGRqlNERES8d+eZw9lXUctT72/wuhTPRbLnbCKwxjm3zjlXA8wHZrTZ5zLgD865TQDOuR0RrEdERESi1JjBvTjzmGzmLVzH/sqe3XsWyXCWC2xusV4c3NbScKC3mRWZ2cdmdlWL1xzw9+D2myJYp4iIiESBO84soKSqjsffXe91KZ6Kj+B7WzvbXDufPw44A0gGFpnZB865VcA3nHNbzaw/8IaZrXTOLTzgQwLB7SaA7OxsioqKwvk1HKCsrCzin9GTqD3DT20afmrT8FJ7hl93atNx2T7mvbOaAreFtIT2okTked2ekQxnxcDgFuuDgK3t7LPLOVcOlJvZQmAMsMo5txUCpzrN7I8ETpMeEM6cc/OAeQDjx493hYWF4f46WikqKiLSn9GTqD3DT20afmrT8FJ7hl93atOBI0s5+7cLWeEG8oPCkZ7U4HV7RvK05kdAgZnlm1kCcAnwSpt9/gycYmbxZpYCTAJWmFmqmaUDmFkqMB1YGsFaRUREJAqMGJDOt44byBPvbWBPeY3X5XgiYuHMOVcH3Aq8DqwAXnTOLTOzWWY2K7jPCuBvwBfAYuBR59xSIBt418w+D27/i3Pub5GqVURERKLHHWcWUFVbzyML13pdiicieVoT59xrwGttts1ts/5r4Ndttq0jcHpTREREephh/dOZMTaXp9/fyA1ThtIvPdHrkrqU7hAgIiIiUee2MwqoqW9g7js9r/dM4UxERESiTn5WKt8+IZdnP9jI9pIqr8vpUgpnIiIiEpVuO72A+gbHQ2+v8bqULqVwJiIiIlFpSN8UZo4fxPOLN7N1X6XX5XQZhTMRERGJWt+bOgyHY04P6j1TOBMREZGoNah3CpdMGMKLSzazeU+F1+V0CYUzERERiWrfnXo0ZsaDb/WM3jOFMxEREYlqAzOTuWziEF7+pJiNu8u9LifiFM5EREQk6n238Gji44zZb3b/3jOFMxEREYl6/TOSuGryUfzx02LW7izzupyIUjgTERGRmHDzaUeTGO9j9purvS4lohTOREREJCZkpSVy9cl5vPL5VlZvL/W6nIhROBMREZGYcfOpQ0nx+3hgQfftPVM4ExERkZjROzWB66bk85cvt7FiW4nX5USEwpmIiIjElBumDCU9KZ4HFqzyupSIUDgTERGRmJKZ4ueGKUN5fdl2lm7Z73U5YadwJiIiIjHn2il5ZCb7uf+N7td7pnAmIiIiMScjyc9Npw7lzZU7+GzzPq/LCSuFMxEREYlJV5+cR5/UBO7rZr1nCmciIiISk9IS47n51KEsXLWTJRv2eF1O2CiciYiISMy6cvJRZKUlcH83unJT4UxERERiVkpCPLcUDuO9Nbv5YN1ur8sJC4UzERERiWmXTxpC//RE7ntjFc45r8s5YgpnIiIiEtOS/D6+N3UYi9fv4f21sd97pnAmIiIiMe/iCYMZmJnULXrPFM5EREQk5iX5fdx6+jA+3riXd1bt9LqcI6JwJiIiIt3CzHGDye2VzP0x3numcCYiIiLdQkJ8HLedMYzPi/fz1sodXpdz2BTOREREpNu48MRBHNU3JabHnimciYiISLfh98Vx2+kFLNtawuvLtntdzmFROBMREZFuZcbYHIZmpfLAglU0NMRe75nCmYiIiHQr8b44bj+zgJVfl/La0m1el9NpCmciIiLS7Zx7fA4F/dN4YMFq6mOs90zhTERERLodX5xxx5nDWbOjjP/7YqvX5XSKwpmIiIh0S+ccO4CRA9J5YMFq6uobvC4nZApnIiIi0i3FxRl3ThvO+l3l/Omz2Ok9UzgTERGRbmv6qGyOzc1g9purqY2R3jOFMxEREem2zIy7pg1n054K/vBJsdflhEThTERERLq1qSP6M2ZwL2a/uYaauujvPVM4ExERkW6tsfdsy75KXlyy2etyDknhTERERLq9UwuyGHdUb+a8vYaq2nqvyzkohTMRERHp9syMf5o2nG37q5i/eJPX5RyUwpmIiIj0CJOP7suk/D7MKVob1b1nCmciIiLSIzSOPdtZWs2zH2z0upwORTScmdnZZvaVma0xsx91sE+hmX1mZsvM7J3OHCsiIiLSGZOG9mXKsCzmvrOWipo6r8tpV8TCmZn5gDnAOcAo4FIzG9Vmn17AQ8D5zrnRwMxQjxURERE5HHdOK2BXWQ1PL4rO3rNI9pxNBNY459Y552qA+cCMNvtcBvzBObcJwDm3oxPHioiIiHTauKP6cNrwfjzyzlrKqqOv9yyS4SwXaDmZSHFwW0vDgd5mVmRmH5vZVZ04VkREROSw3DVtOHsrannyvfVel3KA+Ai+t7WzzbXz+eOAM4BkYJGZfRDisYEPMbsJuAkgOzuboqKiw603JGVlZRH/jJ5E7Rl+atPwU5uGl9oz/NSmh2dsPx8PvbWKofXFpPibo4fX7RnJcFYMDG6xPghoe0v4YmCXc64cKDezhcCYEI8FwDk3D5gHMH78eFdYWBiW4jtSVFREpD+jJ1F7hp/aNPzUpuGl9gw/tenhySrYz7n/8y6rLJc7Coc3bfe6PSN5WvMjoMDM8s0sAbgEeKXNPn8GTjGzeDNLASYBK0I8VkREROSwHZubydmjB/DYP9azr6LG63KaRCycOefqgFuB1wkErhedc8vMbJaZzQruswL4G/AFsBh41Dm3tKNjI1WriIiI9Ex3TCugtLqOR/8RPWPPInlaE+fca8BrbbbNbbP+a+DXoRwrIiIiEk4jB2TwreMH8sR767luSj59UhO8Lkl3CBAREZGe7c4zC6iorWfewnVelwIonImIiEgPN6x/OjPG5PDU+xvYWVrtdTkKZyIiIiK3nVFAdV09j7yz1utSFM5EREREhvZL48ITB/HMBxvZV9XgaS0KZyIiIiLAbacXUNfg+L91tZ7WoXAmIiIiAgzpm8LMcYNYuKWO/ZXeBbSITqUhIiIiEkvuOHM4YxJ3kZns96wG9ZyJiIiIBA3ITGJgmrfxSOFMREREJIoonImIiIhEEYUzERERkSiicCYiIiISRRTORERERKKIwpmIiIhIFFE4ExEREYkiCmciIiIiUUThTERERCSKKJyJiIiIRBFzznldQ9iY2U5gY4Q/JgvYFeHP6EnUnuGnNg0/tWl4qT3DT20aXl3Vnkc55/q13ditwllXMLMlzrnxXtfRXag9w09tGn5q0/BSe4af2jS8vG5PndYUERERiSIKZyIiIiJRROGs8+Z5XUA3o/YMP7Vp+KlNw0vtGX5q0/DytD015kxEREQkiqjnTERERCSK9OhwZmZnm9lXZrbGzH7UzutmZrODr39hZice6lgzm2lmy8yswcx69JUzR9i+j5vZDjNb2rVVx44Q2nekmS0ys2oz+2cvaox1+jk8cu21oZn1MbM3zGx18LG3lzVGu862oZndHfy98JWZneVN1dEnXO1oZuPM7Mvga7PNzMJda48NZ2bmA+YA5wCjgEvNbFSb3c4BCoLLTcDDIRy7FLgQWBjpryGaHUn7Bj0JnB35SmNTiO27B7gNuLeLy+tOnkQ/h0fqSQ5swx8BbzrnCoA3g+vSsScJsQ2DvwcuAUYHj3ko+PtCwteODxP4m9X49yvsvyN6bDgDJgJrnHPrnHM1wHxgRpt9ZgBPu4APgF5mNvBgxzrnVjjnvuq6LyNqHUn74pxbSCBcSPsO2b7OuR3OuY+AWi8K7A70c3jkOmjDGcBTwedPARd0ZU2xppNtOAOY75yrds6tB9YQ+H3R44WjHYN/ozKcc4tcYND+00Tg57cnh7NcYHOL9eLgtlD2CeXYnu5I2lcOTW0nsSzbObcNIPjY3+N6YlFHbajfDZ3T2XbMDT5vuz2senI4a+8ccdtLVzvaJ5Rje7ojaV85NLWdiLRHvxvCw9O//z05nBUDg1usDwK2hrhPKMf2dEfSvnJoajuJZdsbhzAEH3d4XE8s6qgN9buhczrbjsXB5223h1VPDmcfAQVmlm9mCQQG/r3SZp9XgKuCVxWeBOwPdnuGcmxPdyTtK4emn0GJZa8AVwefXw382cNaYlVHbfgKcImZJZpZPoEB64s9qC9WdKodg3+jSs3spOBVmlcRiZ9f51yPXYBvAquAtcCPg9tmAbOCz43AFXFrgS+B8Qc7Nrj92wSSdTWwHXjd668zRtv3eWAbgcHsxcD1Xn890baE0L4Dgm1XAuwLPs/wuu5YWvRzGJk2BPoSuDJudfCxj9d1RvPS2TYEfhz8vfAVcI7X9UfLEq52BMYTmJlhLfAgwQn9w7noDgEiIiIiUaQnn9YUERERiToKZyIiIiJRROFMREREJIoonImIiIhEEYUzERERkSiicCYiPZ6ZbTCzrCPdR0QkHBTORERERKKIwpmI9Chm9icz+9jMlpnZTW1eyzOzlWb2lJl9YWYvm1lKi12+b2afmNmXZjYyeMxEM3vfzD4NPo7o0i9IRLodhTMR6Wmuc86NIzDL921m1rfN6yOAec654wncXeG7LV7b5Zw7EXgY+OfgtpXAqc65E4CfAD+PaPUi0u0pnIlIT3ObmX0OfEDgxsYFbV7f7Jx7L/j8WWBKi9f+EHz8GMgLPs8EXjKzpcD9wOhIFC0iPYfCmYj0GGZWCJwJTHbOjQE+BZLa7Nb2nnYt16uDj/VAfPD5T4G3nXPHAue1834iIp2icCYiPUkmsNc5VxEcM3ZSO/sMMbPJweeXAu+G8J5bgs+vCUuVItKjKZyJSE/yNyDezL4g0OP1QTv7rACuDu7Th8D4soP5FfALM3sP8IWzWBHpmcy5tj34IiI9k5nlAf8XPEUpIuIJ9ZyJiIiIRBH1nImIiIhEEfWciYiIiEQRhTMRERGRKKJwJiIiIhJFFM5EREREoojCmYiIiEgUUTgTERERiSL/P2rm3pG+0vCGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5)) # 가로,세로 비율\n",
    "plt.plot(lasso_score, label='Lasso')\n",
    "plt.plot(ridge_score, label='Ridge')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R square score')\n",
    "plt.xticks(range(7),['0.001','0.01','0.1','1','10','100','1000'])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31feb282",
   "metadata": {},
   "source": [
    "### 가중치 값 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f6e50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns # 182개의 컬럼이름 가져오기\n",
    "# 모델생성\n",
    "lasso = Lasso(alpha=1000)\n",
    "ridge = Ridge(alpha=1000)\n",
    "#모델학습\n",
    "lasso.fit(X_train,y_train)\n",
    "ridge.fit(X_train,y_train)\n",
    "#가중치 가져오기\n",
    "lasso_coef = lasso.coef_\n",
    "ridge_coef = ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f7c27b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.234809e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.033582e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.304783e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.386286e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.142368e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.602003e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.211991e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-6.122724e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.920124e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.453845e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.052289e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.280322e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>7.330037e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.296138e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.699899e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.738351e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.604709e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-9.639726e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x RM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.935128e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-4.392050e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-5.799447e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x RAD</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-3.154298e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x TAX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.672760e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.599010e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x B</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>9.833947e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.366723e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x CRIM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.699899e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.162637e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.242979e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.782172e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x NOX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.728531e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.176960e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.968722e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.338538e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000508e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x TAX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.892362e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>9.062151e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x B</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>-6.517845e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-3.725568e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.738351e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x ZN</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.242979e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.100222e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.829228e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.346727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x RM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.054558e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.257473e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.172787e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.031611e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x TAX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.204247e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.571829e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x B</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.989192e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.342701e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x CRIM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.604709e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.782172e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x INDUS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.829228e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.386286e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x NOX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.563300e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.631744e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.581632e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x DIS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.505309e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.763680e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x TAX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.729545e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x PTRATIO</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.079843e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.233708e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS x LSTAT</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.574602e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-9.639726e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.728531e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.346727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.563300e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.127380e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.051511e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>7.760020e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.845565e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.803860e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x TAX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.560598e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.071314e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.054594e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.116940e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.935128e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.176960e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.054558e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.631744e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x NOX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.051511e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.838081e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.581162e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.172448e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.961941e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x TAX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.111037e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x PTRATIO</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.986696e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.795116e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-4.769329e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-4.392050e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.968722e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.257473e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.581632e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>7.760020e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.581162e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.345145e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.667557e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.592268e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x TAX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.152270e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.753266e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x B</th>\n",
       "      <td>0.000111</td>\n",
       "      <td>-4.419790e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.586671e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-5.799447e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x ZN</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.338538e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.172787e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.505309e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.845565e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x RM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.172448e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.667557e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.330709e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x RAD</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.339250e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x TAX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.791167e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.974543e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x B</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.209640e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.985073e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-3.154298e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x ZN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000508e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x INDUS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.031611e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.763680e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x NOX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.803860e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.961941e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.592268e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.339250e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.053151e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x TAX</th>\n",
       "      <td>0.000130</td>\n",
       "      <td>2.206673e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x PTRATIO</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.939057e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.302972e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.745908e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.672760e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x ZN</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.892362e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x INDUS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.204247e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.729545e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.560598e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.111037e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x AGE</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.152270e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.791167e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.206673e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x TAX</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>-3.267730e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x PTRATIO</th>\n",
       "      <td>-0.001000</td>\n",
       "      <td>3.079417e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.483761e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX x LSTAT</th>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-1.070066e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.599010e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x ZN</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>9.062151e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.571829e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.079843e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.071314e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.986696e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.753266e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.974543e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.939057e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x TAX</th>\n",
       "      <td>-0.000173</td>\n",
       "      <td>3.079417e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-4.166487e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x B</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.016127e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO x LSTAT</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.858749e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>9.833947e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x ZN</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>-6.517845e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.989192e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.233708e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x NOX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.054594e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x RM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.795116e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x AGE</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>-4.419790e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-2.209640e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x RAD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.302972e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x TAX</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.483761e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.016127e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x B</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>3.088562e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B x LSTAT</th>\n",
       "      <td>-0.001905</td>\n",
       "      <td>-3.168540e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x CRIM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.366723e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x ZN</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-3.725568e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x INDUS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.342701e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x CHAS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.574602e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.116940e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x RM</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-4.769329e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x AGE</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.586671e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x DIS</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.985073e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x RAD</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.745908e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x TAX</th>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-1.070066e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x PTRATIO</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>5.858749e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x B</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-3.168540e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT x LSTAT</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.632099e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      lasso         ridge\n",
       "CRIM              -0.000000  1.234809e-03\n",
       "ZN                 0.000000 -5.033582e-02\n",
       "INDUS             -0.000000  6.304783e-03\n",
       "CHAS               0.000000 -8.386286e-04\n",
       "NOX               -0.000000  1.142368e-03\n",
       "RM                 0.000000  1.602003e-02\n",
       "AGE                0.000000  6.211991e-03\n",
       "DIS               -0.000000 -6.122724e-03\n",
       "RAD                0.000000  7.920124e-03\n",
       "TAX               -0.000000  6.453845e-04\n",
       "PTRATIO           -0.000000 -1.052289e-02\n",
       "B                  0.000000  6.280322e-02\n",
       "LSTAT             -0.000000  7.330037e-03\n",
       "CRIM x CRIM       -0.000000  3.296138e-03\n",
       "CRIM x ZN          0.000000  5.699899e-02\n",
       "CRIM x INDUS      -0.000000  2.738351e-02\n",
       "CRIM x CHAS        0.000000  3.604709e-02\n",
       "CRIM x NOX        -0.000000 -9.639726e-03\n",
       "CRIM x RM         -0.000000  4.935128e-02\n",
       "CRIM x AGE        -0.000000 -4.392050e-03\n",
       "CRIM x DIS        -0.000000 -5.799447e-02\n",
       "CRIM x RAD        -0.000000 -3.154298e-02\n",
       "CRIM x TAX        -0.000000  1.672760e-04\n",
       "CRIM x PTRATIO    -0.000000  1.599010e-03\n",
       "CRIM x B          -0.000000  9.833947e-05\n",
       "CRIM x LSTAT      -0.000000  5.366723e-03\n",
       "ZN x CRIM          0.000000  5.699899e-02\n",
       "ZN x ZN            0.000000 -1.162637e-04\n",
       "ZN x INDUS        -0.000000  6.242979e-04\n",
       "ZN x CHAS          0.000000  7.782172e-03\n",
       "ZN x NOX           0.000000 -6.728531e-03\n",
       "ZN x RM            0.000000  3.176960e-02\n",
       "ZN x AGE           0.000000  6.968722e-05\n",
       "ZN x DIS          -0.000000  3.338538e-04\n",
       "ZN x RAD           0.000000 -1.000508e-03\n",
       "ZN x TAX          -0.000000  2.892362e-04\n",
       "ZN x PTRATIO      -0.000000  9.062151e-04\n",
       "ZN x B             0.000032 -6.517845e-04\n",
       "ZN x LSTAT        -0.000000 -3.725568e-03\n",
       "INDUS x CRIM      -0.000000  2.738351e-02\n",
       "INDUS x ZN        -0.000000  6.242979e-04\n",
       "INDUS x INDUS     -0.000000  2.100222e-02\n",
       "INDUS x CHAS       0.000000 -2.829228e-02\n",
       "INDUS x NOX       -0.000000  1.346727e-02\n",
       "INDUS x RM        -0.000000  1.054558e-02\n",
       "INDUS x AGE       -0.000000  4.257473e-03\n",
       "INDUS x DIS       -0.000000  4.172787e-03\n",
       "INDUS x RAD        0.000000  1.031611e-02\n",
       "INDUS x TAX        0.000000  2.204247e-04\n",
       "INDUS x PTRATIO   -0.000000 -2.571829e-02\n",
       "INDUS x B         -0.000000 -2.989192e-04\n",
       "INDUS x LSTAT     -0.000000 -1.342701e-02\n",
       "CHAS x CRIM        0.000000  3.604709e-02\n",
       "CHAS x ZN          0.000000  7.782172e-03\n",
       "CHAS x INDUS       0.000000 -2.829228e-02\n",
       "CHAS x CHAS        0.000000 -8.386286e-04\n",
       "CHAS x NOX         0.000000 -2.563300e-03\n",
       "CHAS x RM          0.000000 -3.631744e-02\n",
       "CHAS x AGE         0.000000  1.581632e-02\n",
       "CHAS x DIS         0.000000  9.505309e-03\n",
       "CHAS x RAD         0.000000  3.763680e-02\n",
       "CHAS x TAX         0.000000 -2.729545e-03\n",
       "CHAS x PTRATIO     0.000000  1.079843e-02\n",
       "CHAS x B           0.000000 -2.233708e-04\n",
       "CHAS x LSTAT       0.000000  2.574602e-02\n",
       "NOX x CRIM        -0.000000 -9.639726e-03\n",
       "NOX x ZN           0.000000 -6.728531e-03\n",
       "NOX x INDUS       -0.000000  1.346727e-02\n",
       "NOX x CHAS         0.000000 -2.563300e-03\n",
       "NOX x NOX         -0.000000  1.127380e-03\n",
       "NOX x RM           0.000000  1.051511e-02\n",
       "NOX x AGE         -0.000000  7.760020e-03\n",
       "NOX x DIS         -0.000000  4.845565e-03\n",
       "NOX x RAD          0.000000 -1.803860e-02\n",
       "NOX x TAX         -0.000000 -2.560598e-02\n",
       "NOX x PTRATIO     -0.000000  5.071314e-03\n",
       "NOX x B            0.000000  3.054594e-03\n",
       "NOX x LSTAT       -0.000000  5.116940e-02\n",
       "RM x CRIM         -0.000000  4.935128e-02\n",
       "RM x ZN            0.000000  3.176960e-02\n",
       "RM x INDUS        -0.000000  1.054558e-02\n",
       "RM x CHAS          0.000000 -3.631744e-02\n",
       "RM x NOX           0.000000  1.051511e-02\n",
       "RM x RM            0.000000  1.838081e-01\n",
       "RM x AGE           0.000000  2.581162e-02\n",
       "RM x DIS          -0.000000  3.172448e-02\n",
       "RM x RAD           0.000000 -6.961941e-02\n",
       "RM x TAX           0.000000 -7.111037e-03\n",
       "RM x PTRATIO       0.000000  3.986696e-02\n",
       "RM x B             0.000000  5.795116e-03\n",
       "RM x LSTAT        -0.000000 -4.769329e-02\n",
       "AGE x CRIM        -0.000000 -4.392050e-03\n",
       "AGE x ZN           0.000000  6.968722e-05\n",
       "AGE x INDUS       -0.000000  4.257473e-03\n",
       "AGE x CHAS         0.000000  1.581632e-02\n",
       "AGE x NOX         -0.000000  7.760020e-03\n",
       "AGE x RM           0.000000  2.581162e-02\n",
       "AGE x AGE          0.000000 -1.345145e-04\n",
       "AGE x DIS         -0.000000  6.667557e-03\n",
       "AGE x RAD          0.000000  4.592268e-03\n",
       "AGE x TAX          0.000000 -1.152270e-04\n",
       "AGE x PTRATIO     -0.000000 -2.753266e-03\n",
       "AGE x B            0.000111 -4.419790e-04\n",
       "AGE x LSTAT       -0.000000 -1.586671e-03\n",
       "DIS x CRIM        -0.000000 -5.799447e-02\n",
       "DIS x ZN          -0.000000  3.338538e-04\n",
       "DIS x INDUS       -0.000000  4.172787e-03\n",
       "DIS x CHAS         0.000000  9.505309e-03\n",
       "DIS x NOX         -0.000000  4.845565e-03\n",
       "DIS x RM          -0.000000  3.172448e-02\n",
       "DIS x AGE         -0.000000  6.667557e-03\n",
       "DIS x DIS         -0.000000  1.330709e-01\n",
       "DIS x RAD         -0.000000 -1.339250e-02\n",
       "DIS x TAX         -0.000000 -1.791167e-03\n",
       "DIS x PTRATIO     -0.000000 -2.974543e-02\n",
       "DIS x B           -0.000000 -2.209640e-03\n",
       "DIS x LSTAT       -0.000000  1.985073e-02\n",
       "RAD x CRIM        -0.000000 -3.154298e-02\n",
       "RAD x ZN           0.000000 -1.000508e-03\n",
       "RAD x INDUS        0.000000  1.031611e-02\n",
       "RAD x CHAS         0.000000  3.763680e-02\n",
       "RAD x NOX          0.000000 -1.803860e-02\n",
       "RAD x RM           0.000000 -6.961941e-02\n",
       "RAD x AGE          0.000000  4.592268e-03\n",
       "RAD x DIS         -0.000000 -1.339250e-02\n",
       "RAD x RAD          0.000000 -1.053151e-01\n",
       "RAD x TAX          0.000130  2.206673e-03\n",
       "RAD x PTRATIO      0.000000  2.939057e-02\n",
       "RAD x B            0.000000 -3.302972e-04\n",
       "RAD x LSTAT       -0.000000 -1.745908e-02\n",
       "TAX x CRIM        -0.000000  1.672760e-04\n",
       "TAX x ZN          -0.000000  2.892362e-04\n",
       "TAX x INDUS        0.000000  2.204247e-04\n",
       "TAX x CHAS         0.000000 -2.729545e-03\n",
       "TAX x NOX         -0.000000 -2.560598e-02\n",
       "TAX x RM           0.000000 -7.111037e-03\n",
       "TAX x AGE          0.000000 -1.152270e-04\n",
       "TAX x DIS         -0.000000 -1.791167e-03\n",
       "TAX x RAD          0.000000  2.206673e-03\n",
       "TAX x TAX          0.000018 -3.267730e-05\n",
       "TAX x PTRATIO     -0.001000  3.079417e-03\n",
       "TAX x B            0.000000  1.483761e-05\n",
       "TAX x LSTAT       -0.000306 -1.070066e-04\n",
       "PTRATIO x CRIM    -0.000000  1.599010e-03\n",
       "PTRATIO x ZN      -0.000000  9.062151e-04\n",
       "PTRATIO x INDUS   -0.000000 -2.571829e-02\n",
       "PTRATIO x CHAS     0.000000  1.079843e-02\n",
       "PTRATIO x NOX     -0.000000  5.071314e-03\n",
       "PTRATIO x RM       0.000000  3.986696e-02\n",
       "PTRATIO x AGE     -0.000000 -2.753266e-03\n",
       "PTRATIO x DIS     -0.000000 -2.974543e-02\n",
       "PTRATIO x RAD      0.000000  2.939057e-02\n",
       "PTRATIO x TAX     -0.000173  3.079417e-03\n",
       "PTRATIO x PTRATIO -0.000000 -4.166487e-02\n",
       "PTRATIO x B       -0.000000 -1.016127e-03\n",
       "PTRATIO x LSTAT   -0.000000  5.858749e-03\n",
       "B x CRIM          -0.000000  9.833947e-05\n",
       "B x ZN             0.000094 -6.517845e-04\n",
       "B x INDUS         -0.000000 -2.989192e-04\n",
       "B x CHAS           0.000000 -2.233708e-04\n",
       "B x NOX            0.000000  3.054594e-03\n",
       "B x RM             0.000000  5.795116e-03\n",
       "B x AGE            0.000064 -4.419790e-04\n",
       "B x DIS           -0.000000 -2.209640e-03\n",
       "B x RAD            0.000000 -3.302972e-04\n",
       "B x TAX            0.000008  1.483761e-05\n",
       "B x PTRATIO       -0.000000 -1.016127e-03\n",
       "B x B              0.000054  3.088562e-07\n",
       "B x LSTAT         -0.001905 -3.168540e-05\n",
       "LSTAT x CRIM      -0.000000  5.366723e-03\n",
       "LSTAT x ZN        -0.000000 -3.725568e-03\n",
       "LSTAT x INDUS     -0.000000 -1.342701e-02\n",
       "LSTAT x CHAS       0.000000  2.574602e-02\n",
       "LSTAT x NOX       -0.000000  5.116940e-02\n",
       "LSTAT x RM        -0.000000 -4.769329e-02\n",
       "LSTAT x AGE       -0.000000 -1.586671e-03\n",
       "LSTAT x DIS       -0.000000  1.985073e-02\n",
       "LSTAT x RAD       -0.000000 -1.745908e-02\n",
       "LSTAT x TAX       -0.000013 -1.070066e-04\n",
       "LSTAT x PTRATIO   -0.000000  5.858749e-03\n",
       "LSTAT x B         -0.000004 -3.168540e-05\n",
       "LSTAT x LSTAT      0.000000  2.632099e-02"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',200) # pandas 최대 rows 증가\n",
    "coef_df = pd.DataFrame([lasso_coef,ridge_coef],\n",
    "                      columns=cols,\n",
    "                      index=['lasso','ridge']).T\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc363bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c3082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a8cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f72c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252672b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41611252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668bc94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a48caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07249d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c830270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0d145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0178d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a746ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894db329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea8750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a36b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45405ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7817f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d695f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22302f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff66c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7718ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409e832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f10446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62caf4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
